{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"hwk3_datasets/yelp-train.txt\",delimiter='\\t', dtype=str, header=None, names = ['Review','Score'])\n",
    "test = pd.read_csv(\"hwk3_datasets/yelp-test.txt\",delimiter='\\t', dtype = str, header=None, names = ['Review','Score'])\n",
    "val = pd.read_csv(\"hwk3_datasets/yelp-valid.txt\",delimiter='\\t', dtype = str, header = None, names = ['Review','Score'])\n",
    "\n",
    "transtab = str.maketrans('','', string.punctuation)\n",
    "\n",
    "train_review_len = np.zeros(len(train['Review']))\n",
    "test_review_len = np.zeros(len(test['Review']))\n",
    "val_review_len = np.zeros(len(val['Review']))\n",
    "\n",
    "for i,ex in enumerate(train['Review']):\n",
    "    train['Review'][i] = ex.translate(transtab)\n",
    "    train['Review'][i] = (train['Review'][i]).lower()\n",
    "for i,ex in enumerate(test['Review']):\n",
    "    test['Review'][i] = ex.translate(transtab) \n",
    "    test['Review'][i] = test['Review'][i].lower()\n",
    "for i,ex in enumerate(val['Review']):\n",
    "    val['Review'][i] = ex.translate(transtab) \n",
    "    val['Review'][i] = val['Review'][i].lower()\n",
    "    \n",
    "train['Score']=train['Score'].astype(np.int32)\n",
    "test['Score']=test['Score'].astype(np.int32)\n",
    "val['Score']=val['Score'].astype(np.int32)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features = 10000) \n",
    "\n",
    "train_vectors_notnorm = vectorizer.fit_transform(train['Review'])\n",
    "test_vectors_notnorm = vectorizer.transform(test['Review'])\n",
    "val_vectors_notnorm = vectorizer.transform(val['Review'])      \n",
    "\n",
    "normalizer = Normalizer(norm='l2')\n",
    "\n",
    "train_vectors = normalizer.transform(train_vectors_notnorm)\n",
    "test_vectors = normalizer.transform(test_vectors_notnorm)\n",
    "val_vectors = normalizer.transform(val_vectors_notnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(classifier):    \n",
    "    try:\n",
    "        classifier.fit(train_vectors, train['Score'])\n",
    "        train_y =classifier.predict(train_vectors)\n",
    "        test_y = classifier.predict(test_vectors)\n",
    "        val_y = classifier.predict(val_vectors)\n",
    "       \n",
    "    except:\n",
    "        classifier.fit(train_vectors.toarray(), train['Score'])\n",
    "        train_y =classifier.predict(train_vectors.toarray())\n",
    "        test_y = classifier.predict(test_vectors.toarray())\n",
    "        val_y = classifier.predict(val_vectors.toarray())        \n",
    "        \n",
    "    train_f1 = f1_score(train['Score'],train_y, average='macro')\n",
    "    val_f1 = f1_score(val['Score'],val_y, average='macro')\n",
    "    test_f1 = f1_score(test['Score'],test_y, average='macro')\n",
    "    \n",
    "    print(type(classifier))\n",
    "    print(f\"Train F1: {train_f1}\")\n",
    "    print(f\"Val F1: {val_f1}\")\n",
    "    print(f\"Test F1: {test_f1}\")\n",
    "    print(\"\\n\")\n",
    "    return val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier_quick(classifier):    \n",
    "    try:\n",
    "        classifier.fit(train_vectors, train['Score'])\n",
    "        val_y = classifier.predict(val_vectors)        \n",
    "    except:\n",
    "        classifier.fit(train_vectors.toarray(), train['Score'])\n",
    "        val_y = classifier.predict(val_vectors.toarray())          \n",
    "        \n",
    "    val_f1 = f1_score(val['Score'],val_y, average='macro')    \n",
    "    return val_f1\n",
    "    \n",
    "def test_classifier(classifier):    \n",
    "    try:\n",
    "        classifier.fit(train_vectors, train['Score'])\n",
    "        test_y = classifier.predict(test_vectors)        \n",
    "    except:\n",
    "        classifier.fit(train_vectors.toarray(), train['Score'])\n",
    "        test_y = classifier.predict(test_vectors.toarray())          \n",
    "        \n",
    "    test_f1 = f1_score(test['Score'],test_y, average='macro')    \n",
    "    return test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_params = ParameterGrid({})\n",
    "tree_params = ParameterGrid({'random_state':[10],'max_depth':[None,10,100,1000],'min_samples_split':[2,5,10]})\n",
    "svm_params = ParameterGrid({'random_state':[10],'loss':['hinge','squared_hinge'],'C':[.5,1.0,2.0]})\n",
    "\n",
    "classifiers= [(GaussianNB, bayes_params), (DecisionTreeClassifier, tree_params), (svm.LinearSVC, svm_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best params for a classifier \n",
    "def best_parameters(classifier, param_grid):\n",
    "    best_Score=0 #f1 score on validation\n",
    "    best_params=None\n",
    "    for params in param_grid:\n",
    "        print(f\"Trying: {params}\")\n",
    "        Score = eval_classifier_quick(classifier(**params))\n",
    "        print(f\"F1 Score: {Score}\\n\")\n",
    "        if Score>best_Score:\n",
    "            best_Score=Score\n",
    "            best_params=params       \n",
    "            \n",
    "    print(f\"Best params: {best_params}\")\n",
    "    print(f\"Best F1 Score: {best_Score}\\n\")\n",
    "    return classifier(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.naive_bayes.GaussianNB'>\n",
      "Trying: {}\n",
      "F1 Score: 0.2351740905717059\n",
      "\n",
      "Best params: {}\n",
      "Best F1 Score: 0.2351740905717059\n",
      "\n",
      "Test score for best params: 0.23929827431059195\n",
      "\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "Trying: {'max_depth': None, 'min_samples_split': 2, 'random_state': 10}\n",
      "F1 Score: 0.28967928174667035\n",
      "\n",
      "Trying: {'max_depth': None, 'min_samples_split': 5, 'random_state': 10}\n",
      "F1 Score: 0.2897887503244254\n",
      "\n",
      "Trying: {'max_depth': None, 'min_samples_split': 10, 'random_state': 10}\n",
      "F1 Score: 0.2803758107648058\n",
      "\n",
      "Trying: {'max_depth': 10, 'min_samples_split': 2, 'random_state': 10}\n",
      "F1 Score: 0.30768505050459105\n",
      "\n",
      "Trying: {'max_depth': 10, 'min_samples_split': 5, 'random_state': 10}\n",
      "F1 Score: 0.3092932288750916\n",
      "\n",
      "Trying: {'max_depth': 10, 'min_samples_split': 10, 'random_state': 10}\n",
      "F1 Score: 0.30760113871869177\n",
      "\n",
      "Trying: {'max_depth': 100, 'min_samples_split': 2, 'random_state': 10}\n",
      "F1 Score: 0.28967928174667035\n",
      "\n",
      "Trying: {'max_depth': 100, 'min_samples_split': 5, 'random_state': 10}\n",
      "F1 Score: 0.2897887503244254\n",
      "\n",
      "Trying: {'max_depth': 100, 'min_samples_split': 10, 'random_state': 10}\n",
      "F1 Score: 0.2803758107648058\n",
      "\n",
      "Trying: {'max_depth': 1000, 'min_samples_split': 2, 'random_state': 10}\n",
      "F1 Score: 0.28967928174667035\n",
      "\n",
      "Trying: {'max_depth': 1000, 'min_samples_split': 5, 'random_state': 10}\n",
      "F1 Score: 0.2897887503244254\n",
      "\n",
      "Trying: {'max_depth': 1000, 'min_samples_split': 10, 'random_state': 10}\n",
      "F1 Score: 0.2803758107648058\n",
      "\n",
      "Best params: {'max_depth': 10, 'min_samples_split': 5, 'random_state': 10}\n",
      "Best F1 Score: 0.3092932288750916\n",
      "\n",
      "Test score for best params: 0.2628783432037355\n",
      "\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "Trying: {'C': 0.5, 'loss': 'hinge', 'random_state': 10}\n",
      "F1 Score: 0.39765691920591173\n",
      "\n",
      "Trying: {'C': 0.5, 'loss': 'squared_hinge', 'random_state': 10}\n",
      "F1 Score: 0.4738197634706494\n",
      "\n",
      "Trying: {'C': 1.0, 'loss': 'hinge', 'random_state': 10}\n",
      "F1 Score: 0.42086757951733383\n",
      "\n",
      "Trying: {'C': 1.0, 'loss': 'squared_hinge', 'random_state': 10}\n",
      "F1 Score: 0.4773903905644473\n",
      "\n",
      "Trying: {'C': 2.0, 'loss': 'hinge', 'random_state': 10}\n",
      "F1 Score: 0.4361949857430775\n",
      "\n",
      "Trying: {'C': 2.0, 'loss': 'squared_hinge', 'random_state': 10}\n",
      "F1 Score: 0.4607654797558565\n",
      "\n",
      "Best params: {'C': 1.0, 'loss': 'squared_hinge', 'random_state': 10}\n",
      "Best F1 Score: 0.4773903905644473\n",
      "\n",
      "Test score for best params: 0.4686722893287243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pair in classifiers:\n",
    "    classifier = pair[0]\n",
    "    param_grid = pair[1]\n",
    "    print(classifier)    \n",
    "    best_classifier = best_parameters(classifier,param_grid) \n",
    "    print(f\"Test score for best params: {test_classifier(best_classifier)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
