{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"hwk3_datasets/yelp-train.txt\",delimiter='\\t', dtype=str, header=None, names = ['Review','Score'])\n",
    "test = pd.read_csv(\"hwk3_datasets/yelp-test.txt\",delimiter='\\t', dtype = str, header=None, names = ['Review','Score'])\n",
    "val = pd.read_csv(\"hwk3_datasets/yelp-valid.txt\",delimiter='\\t', dtype = str, header = None, names = ['Review','Score'])\n",
    "\n",
    "transtab = str.maketrans('','', string.punctuation)\n",
    "for i,ex in enumerate(train['Review']):\n",
    "    train['Review'][i] = ex.translate(transtab)\n",
    "    train['Review'][i] = (train['Review'][i]).lower()\n",
    "for i,ex in enumerate(test['Review']):\n",
    "    test['Review'][i] = ex.translate(transtab) \n",
    "    test['Review'][i] = test['Review'][i].lower()\n",
    "for i,ex in enumerate(val['Review']):\n",
    "    val['Review'][i] = ex.translate(transtab) \n",
    "    val['Review'][i] = val['Review'][i].lower()\n",
    "    \n",
    "train['Score']=train['Score'].astype(np.int32)\n",
    "test['Score']=test['Score'].astype(np.int32)\n",
    "val['Score']=val['Score'].astype(np.int32)\n",
    "                 \n",
    "vectorizer = CountVectorizer(max_features = 10000, binary=True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train['Review'])\n",
    "test_vectors = vectorizer.transform(test['Review'])\n",
    "val_vectors = vectorizer.transform(val['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(classifier):    \n",
    "   \n",
    "    try:\n",
    "        classifier.fit(train_vectors, train['Score'])\n",
    "        train_y =classifier.predict(train_vectors)\n",
    "        test_y = classifier.predict(test_vectors)\n",
    "        val_y = classifier.predict(val_vectors)\n",
    "       \n",
    "    except:\n",
    "        classifier.fit(train_vectors.toarray(), train['Score'])\n",
    "        train_y =classifier.predict(train_vectors.toarray())\n",
    "        test_y = classifier.predict(test_vectors.toarray())\n",
    "        val_y = classifier.predict(val_vectors.toarray())        \n",
    "        \n",
    "    train_f1 = f1_score(train['Score'],train_y, average='macro')\n",
    "    val_f1 = f1_score(val['Score'],val_y, average='macro')\n",
    "    test_f1 = f1_score(test['Score'],test_y, average='macro')\n",
    "    \n",
    "    print(type(classifier))\n",
    "    print(f\"Train F1: {train_f1}\")\n",
    "    print(f\"Val F1: {val_f1}\")\n",
    "    print(f\"Test F1: {test_f1}\")\n",
    "    print(\"\\n\")\n",
    "    return val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier_quick(classifier):    \n",
    "    try:\n",
    "        classifier.fit(train_vectors, train['Score'])\n",
    "        val_y = classifier.predict(val_vectors)        \n",
    "    except:\n",
    "        classifier.fit(train_vectors.toarray(), train['Score'])\n",
    "        val_y = classifier.predict(val_vectors.toarray())          \n",
    "        \n",
    "    val_f1 = f1_score(val['Score'],val_y, average='macro')    \n",
    "    return val_f1\n",
    "    \n",
    "def test_classifier(classifier):    \n",
    "    try:\n",
    "        classifier.fit(train_vectors, train['Score'])\n",
    "        test_y = classifier.predict(test_vectors)        \n",
    "    except:\n",
    "        classifier.fit(train_vectors.toarray(), train['Score'])\n",
    "        test_y = classifier.predict(test_vectors.toarray())          \n",
    "        \n",
    "    test_f1 = f1_score(test['Score'],test_y, average='macro')    \n",
    "    return test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score of Random Classifier on Test: 0.17230888773618042\n",
      "\n",
      "F1 Score of Majority Classifier on Test: 0.10392301998519615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "random = DummyClassifier(strategy='uniform', random_state=10)\n",
    "maj = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "print(f\"F1 Score of Random Classifier on Test: {test_classifier(random)}\\n\")\n",
    "print(f\"F1 Score of Majority Classifier on Test: {test_classifier(maj)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_params = ParameterGrid({'alpha':[.01,.1,.5,1]})\n",
    "tree_params = ParameterGrid({'random_state':[10],'max_depth':[None,10,100,1000],'min_samples_split':[2,5,10]})\n",
    "svm_params = ParameterGrid({'random_state':[10],'loss':['hinge','squared_hinge'],'C':[.5,1.0,2.0]})\n",
    "\n",
    "classifiers= [(BernoulliNB, bayes_params), (DecisionTreeClassifier, tree_params), (svm.LinearSVC, svm_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_parameters(classifier, param_grid):\n",
    "    best_Score=0\n",
    "    best_params=None\n",
    "    for params in param_grid:\n",
    "        print(f\"Trying: {params}\")\n",
    "        Score = eval_classifier_quick(classifier(**params))\n",
    "        print(f\"F1 Score Validation: {Score}\\n\")\n",
    "        if Score>best_Score:\n",
    "            best_Score=Score\n",
    "            best_params=params       \n",
    "            \n",
    "    print(f\"Best params for Validation: {best_params}\")\n",
    "    print(f\"Best F1 Score on Validation: {best_Score}\\n\")\n",
    "    return classifier(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.naive_bayes.BernoulliNB'>\n",
      "Trying: {'alpha': 0.01}\n",
      "F1 Score Validation: 0.38051604534431227\n",
      "\n",
      "Trying: {'alpha': 0.1}\n",
      "F1 Score Validation: 0.3721284609021715\n",
      "\n",
      "Trying: {'alpha': 0.5}\n",
      "F1 Score Validation: 0.36264671577037066\n",
      "\n",
      "Trying: {'alpha': 1}\n",
      "F1 Score Validation: 0.3369298973133679\n",
      "\n",
      "Best params for Validation: {'alpha': 0.01}\n",
      "Best F1 Score on Validation: 0.38051604534431227\n",
      "\n",
      "Test score for best params: 0.36572174954585585\n",
      "\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "Trying: {'max_depth': None, 'min_samples_split': 2, 'random_state': 10}\n",
      "F1 Score Validation: 0.25071684622351753\n",
      "\n",
      "Trying: {'max_depth': None, 'min_samples_split': 5, 'random_state': 10}\n",
      "F1 Score Validation: 0.26523413087325115\n",
      "\n",
      "Trying: {'max_depth': None, 'min_samples_split': 10, 'random_state': 10}\n",
      "F1 Score Validation: 0.25921883090992326\n",
      "\n",
      "Trying: {'max_depth': 10, 'min_samples_split': 2, 'random_state': 10}\n",
      "F1 Score Validation: 0.2877110115742789\n",
      "\n",
      "Trying: {'max_depth': 10, 'min_samples_split': 5, 'random_state': 10}\n",
      "F1 Score Validation: 0.2878531996997386\n",
      "\n",
      "Trying: {'max_depth': 10, 'min_samples_split': 10, 'random_state': 10}\n",
      "F1 Score Validation: 0.28863171451423775\n",
      "\n",
      "Trying: {'max_depth': 100, 'min_samples_split': 2, 'random_state': 10}\n",
      "F1 Score Validation: 0.26964379427481255\n",
      "\n",
      "Trying: {'max_depth': 100, 'min_samples_split': 5, 'random_state': 10}\n",
      "F1 Score Validation: 0.26523413087325115\n",
      "\n",
      "Trying: {'max_depth': 100, 'min_samples_split': 10, 'random_state': 10}\n",
      "F1 Score Validation: 0.25921883090992326\n",
      "\n",
      "Trying: {'max_depth': 1000, 'min_samples_split': 2, 'random_state': 10}\n",
      "F1 Score Validation: 0.25071684622351753\n",
      "\n",
      "Trying: {'max_depth': 1000, 'min_samples_split': 5, 'random_state': 10}\n",
      "F1 Score Validation: 0.26523413087325115\n",
      "\n",
      "Trying: {'max_depth': 1000, 'min_samples_split': 10, 'random_state': 10}\n",
      "F1 Score Validation: 0.25921883090992326\n",
      "\n",
      "Best params for Validation: {'max_depth': 10, 'min_samples_split': 10, 'random_state': 10}\n",
      "Best F1 Score on Validation: 0.28863171451423775\n",
      "\n",
      "Test score for best params: 0.27117951516332034\n",
      "\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "Trying: {'C': 0.5, 'loss': 'hinge', 'random_state': 10}\n",
      "F1 Score Validation: 0.42625602744462404\n",
      "\n",
      "Trying: {'C': 0.5, 'loss': 'squared_hinge', 'random_state': 10}\n",
      "F1 Score Validation: 0.43745568878627256\n",
      "\n",
      "Trying: {'C': 1.0, 'loss': 'hinge', 'random_state': 10}\n",
      "F1 Score Validation: 0.42393868743892094\n",
      "\n",
      "Trying: {'C': 1.0, 'loss': 'squared_hinge', 'random_state': 10}\n",
      "F1 Score Validation: 0.4297492672078924\n",
      "\n",
      "Trying: {'C': 2.0, 'loss': 'hinge', 'random_state': 10}\n",
      "F1 Score Validation: 0.41431772340860296\n",
      "\n",
      "Trying: {'C': 2.0, 'loss': 'squared_hinge', 'random_state': 10}\n",
      "F1 Score Validation: 0.41261228660479726\n",
      "\n",
      "Best params for Validation: {'C': 0.5, 'loss': 'squared_hinge', 'random_state': 10}\n",
      "Best F1 Score on Validation: 0.43745568878627256\n",
      "\n",
      "Test score for best params: 0.4026956035166604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pair in classifiers:\n",
    "    classifier = pair[0]\n",
    "    param_grid = pair[1]\n",
    "    print(classifier)    \n",
    "    best_classifier = best_parameters(classifier,param_grid) \n",
    "    print(f\"Test score for best params: {test_classifier(best_classifier)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
